---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# ideadata <img src='man/figures/logo.png' align="right" height="139" /> 

<!-- badges: start -->
<!-- badges: end -->

`ideadata` helps data analysts at IDEA Public Schools  access and use data stored in IDEA's data warehouse.  `ideadata` does so in a [tidyverse](https://www.tidyverse.org/) friendly way.

`ideadata` is an anagram for "data aide".  Hench the logo. Ooooh Yeah!

<img src='https://media3.giphy.com/media/Zx0BgEPhIevlAVEbnq/giphy.gif' align="center" height = "200" />


## Installation


Since `ideadata` is an internal IDEA package there is only a development version, which is installed from [GitHub](https://github.com/) with:

``` {r install, eval=FALSE}
#install.pacakges("remotes")
remotes::install_github("idea-analytics/ideadata")

#renv::install("idea-analytics/ideadata@main") also works
```
## Example

Here's how you connect to a table in the warehouse.

```{r example, eval=TRUE}
library(dplyr)
library(ideadata)

schools <- get_schools()

head(schools)
```

The `schools` object above is `tbl` object.  That means it works with `dplyr` verbs and functions, but  what  happens in the background is that `dplyr` and `dbplyr` generate SQL that is sent to the database you are connected to and that all computation (e.g., filtering, selecting, joining, calculations, aggregation) are completed on the remote SQL Server instance and **not** on your computer.  

Nevertheless, you will eventually want to pull that data down onto your machine when you want to use R or Python do what they can do (like modeling or graphics) that the database can't do. 

Pulling that data down is easy with [dplyr::collect()]

```{r collect}
library(dplyr)

schools_df <- schools %>% 
  collect() %>% 
  janitor::clean_names()
```

(Here `janitor::clean_names()` snake_cases all the column names). 

### What if I am pulling down lots fo data (say, millions of rows)?
In this instance the database connection may fail.  It's not ideal, but it happens.  One way to deal with this is to pull down the data piecemeal.  The `collector()` function in `ideadata` makes this task trivial. It takes one argument, which is a column name form the table you want to pull down, which is used to break up the data into smaller sets of data that are pulled down from the database onto your computer and then recombined into a single table. 

```{r collector}

schools_df <- schools %>% 
  collector(SchoolState, CountyName) %>% 
  janitor::clean_names()

```



